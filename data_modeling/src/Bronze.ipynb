{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65d31900-930b-4469-a110-178419522eb4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "create catalog, schema, landing zone"
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "\n",
    "# create catalog if not exists data_modeling;\n",
    "# use catalog data_modeling;\n",
    "\n",
    "# drop schema if exists raw cascade;\n",
    "# drop schema if exists bronze cascade;\n",
    "# drop schema if exists silver cascade;\n",
    "# drop schema if exists gold cascade;\n",
    "\n",
    "# create schema raw;\n",
    "# create schema bronze;\n",
    "# create schema silver;\n",
    "# create schema gold;\n",
    "\n",
    "# create volume data_modeling.raw.landing_zone;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ef844c2-ba26-46bb-80d3-a040b96eda98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Reference the volume\n",
    "# landing_zone_path = \"/Volumes/data_modeling/raw/landing_zone\"\n",
    "\n",
    "# # Create subdirectories\n",
    "# dbutils.fs.mkdirs(f\"{landing_zone_path}/cust\")\n",
    "# dbutils.fs.mkdirs(f\"{landing_zone_path}/product\")\n",
    "# dbutils.fs.mkdirs(f\"{landing_zone_path}/loc\")\n",
    "# dbutils.fs.mkdirs(f\"{landing_zone_path}/sales\")\n",
    "\n",
    "# # List contents\n",
    "# dbutils.fs.ls(landing_zone_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cde8372b-b082-42ec-b0f4-c8b73bc6c66f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "use catalog"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "use catalog data_modeling;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d60dde41-117a-422d-83dd-6bdd94032046",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "lib"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, current_timestamp, lit, trim\n",
    "from pyspark.sql import DataFrame\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc5876e1-f8b8-445d-9bdf-7e02670d4ca1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "data format"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "date_var = datetime.now().strftime('%Y%m%d')\n",
    "print(date_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92e18f1b-6912-4ae6-be8c-b99f5bee1f50",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "read latest file from landing zone"
    }
   },
   "outputs": [],
   "source": [
    "sales_df = spark.read.format('csv').option('header','true').option('inferSchema','true').load(f'/Volumes/data_modeling/raw/landing_zone/sales_{date_var}.csv')\n",
    "\n",
    "sales_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9383d52b-c831-4bc7-9e52-e45a94108082",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def format_columns_for_delta(df: DataFrame, naming_convention: str = \"snake_case\") -> DataFrame:\n",
    "    \"\"\"Convert DataFrame columns to Delta-compliant format\"\"\"\n",
    "    \n",
    "    def to_snake_case(name: str) -> str:\n",
    "        name = re.sub(r'[\\s\\-]+', '_', name)\n",
    "        name = re.sub(r'[,;{}\\(\\)\\n\\t=]', '', name)\n",
    "        name = re.sub(r'([a-z])([A-Z])', r'\\1_\\2', name)\n",
    "        return name.lower().strip('_')\n",
    "    \n",
    "    def to_camel_case(name: str) -> str:\n",
    "        name = to_snake_case(name)\n",
    "        parts = name.split('_')\n",
    "        return parts[0] + ''.join(word.capitalize() for word in parts[1:])\n",
    "    \n",
    "    def to_pascal_case(name: str) -> str:\n",
    "        \"\"\"Convert to PascalCase - All words capitalized\"\"\"\n",
    "        name = to_snake_case(name)\n",
    "        parts = name.split('_')\n",
    "        return ''.join(word.capitalize() for word in parts)\n",
    "    \n",
    "    def to_lowercase(name: str) -> str:\n",
    "        name = re.sub(r'[\\s\\-]+', '_', name)\n",
    "        name = re.sub(r'[,;{}\\(\\)\\n\\t=]', '', name)\n",
    "        return name.lower().strip('_')\n",
    "    \n",
    "    converters = {\n",
    "        \"snake_case\": to_snake_case,\n",
    "        \"camelCase\": to_camel_case,\n",
    "        \"PascalCase\": to_pascal_case,\n",
    "        \"lowercase\": to_lowercase\n",
    "    }\n",
    "    \n",
    "    converter = converters.get(naming_convention, to_snake_case)\n",
    "    df_formatted = df\n",
    "    \n",
    "    for old_name in df.columns:\n",
    "        new_name = converter(old_name)\n",
    "        if old_name != new_name:\n",
    "            df_formatted = df_formatted.withColumnRenamed(old_name, new_name)\n",
    "    \n",
    "    return df_formatted\n",
    "\n",
    "sales_df = format_columns_for_delta(sales_df, naming_convention=\"PascalCase\")\n",
    "\n",
    "sales_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae199b13-f2af-4853-a8c9-a95e43a95a5c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "write to bronze"
    }
   },
   "outputs": [],
   "source": [
    "sales_df.write.format(\n",
    "    'delta'\n",
    ").mode(\n",
    "    'overwrite'\n",
    ").option(\n",
    "    'overwriteSchema',\n",
    "    'true'\n",
    ").saveAsTable(\n",
    "    'bronze.raw_sales_data'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8481809742974972,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
